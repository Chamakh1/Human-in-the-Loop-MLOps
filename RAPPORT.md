# Project Report: Human-in-the-Loop Annotation and Evaluation Platform

## 1. Project Overview

This project consists of the development of an **interactive Human-in-the-Loop (HITL) web application** dedicated to image annotation and the evaluation of artificial intelligence agent performance, particularly in the context of **object detection**.

The main objective is to effectively integrate human expertise into the lifecycle of an AI system in order to:

- supervise automatic predictions generated by a detection model,
- correct and adjust errors (poorly positioned or incorrect bounding boxes),
- validate or reject AI agent decisions,
- collect structured human feedback to support continuous model improvement.

This Human-in-the-Loop approach helps reduce model bias, improve the quality of annotated data, and ensure greater system reliability in real-world environments.

---

## 2. Current Technical Architecture

The project architecture is based on a clear separation between the user interface (Frontend) and the application logic (Backend), communicating via REST APIs.

### 2.1 Frontend (User Interface)

The frontend is developed using **HTML5 and native JavaScript**, directly leveraging the **Canvas API** for graphical rendering and real-time interaction with images.

#### Interaction Modes

- **Annotate**: manual creation of bounding boxes drawn by the user (red color).
- **Review**: visualization, adjustment, and validation of automatically generated AI predictions (blue color).

#### Key Features

- Smooth navigation between frames (images) with support for automatic mode (*auto-play*).
- Robust **Undo / Redo** system based on a history stack.
- Dynamic management of annotation visibility (visual filtering).
- Accurate coordinate conversion between displayed resolution (canvas) and real image resolution.
- Full mouse interaction: drawing, moving, and resizing bounding boxes.

---

### 2.2 Backend (API Server)

The backend is implemented in **Python using Flask**, exposing a REST API accessible locally (`localhost:5000`).  
It handles data management, annotation persistence, and interfacing with the AI model.

#### Main Endpoints

- `GET /frame/info`  
  Retrieves image metadata and existing annotations.

- `POST /frame/info`  
  Saves created or modified annotations along with human feedback.

- `POST /ai/detect`  
  Triggers an object detection model (local or remote) to generate automatic predictions.

The backend acts as a central point ensuring data consistency between the human user and the AI agent.

---

## 3. Implemented Features

| Feature                     | Description |
|----------------------------|-------------|
| Dynamic annotation          | Manual drawing of bounding boxes with movement and resizing. |
| AI audit                    | Interface to analyze, accept, or reject AI model predictions. |
| Human feedback              | Qualitative rating system (Positive / Negative / Neutral) via keyboard shortcuts (A, S, D). |
| State management            | Action history enabling undo operations and fast error correction. |
| Visual filtering            | Selective hiding or displaying of annotations to improve readability. |

---

## 4. Contribution of Human-in-the-Loop

Integrating humans into the decision-making loop provides several major benefits:

- progressive improvement of annotation quality,
- rapid detection of AI model errors,
- creation of a reliable dataset for retraining,
- increased trust in automated decisions.

Human feedback becomes a strategic asset, transforming a simple annotation tool into a true **continuous learning system**.

---

## 5. Docker + MLflow + Backend Integration: Analysis and Results

This section details the technical integration and presents the analysis of data collected during experimentation sessions.

### 5.1 Containerization and Architecture

The platform is fully **containerized using Docker**. The Flask backend acts as an intermediary: it receives frontend actions and automatically forwards metrics and parameters to the **MLflow** tracking server.

### 5.2 Experiment Tracking

**MLflow** ensures full traceability. Each save operation triggered by the backend creates a new *Run* within the `human_in_the_loop_experiment` experiment.

![MLflow interface listing experiments](/Simple-Human-in-the-Loop-ML-Interface/assets/mlflow%202.png)  
*Figure 1: MLflow dashboard listing the “auto_save_from_backend” runs. It shows session history, duration, and status.*

### 5.3 Results Analysis and Visualizations

The analysis of MLflow-generated graphs provides valuable insights into model performance and annotator behavior.

#### A. AI vs Human Comparison (Model Precision)

One key metric is the difference between the number of boxes detected by the AI (`ai_boxes_count`) and the number of boxes ultimately validated or created by the human (`human_boxes_count`).

![Bar chart comparing AI and Human boxes](/Simple-Human-in-the-Loop-ML-Interface/assets/human%20boxes%20vs%20ia.png)  
*Figure 2: Direct comparison for a single run. The AI proposed more than 20 boxes, while the human retained only 10.*

**Analysis:** This graph highlights the model’s false positive rate. The significant gap suggests that the AI is overly sensitive or detects irrelevant objects that the human must filter out. This is where the HITL loop delivers its full value by cleaning the dataset.

#### B. Session Evolution (Longitudinal View)

We can compare metrics across multiple successive sessions to observe variability in the processed data.

![Comparison of the last 5 runs](/Simple-Human-in-the-Loop-ML-Interface/assets/compare%205%20last%20runs.png)  
*Figure 3: Comparison of recent runs. The brown bar (Total Frames) remains constant, while colored bars (AI Boxes) vary depending on the processed images.*

#### C. Annotation Progress

The parallel coordinates plot visualizes the relationship between the total dataset size and annotation progress.

![Parallel coordinates plot](/Simple-Human-in-the-Loop-ML-Interface/assets/total%20frame%20vs%20evalutaed%20frame.png)  
*Figure 4: Parallel Coordinates Plot linking the total number of frames (~110) to the number of frames actually evaluated (2 to 3).*

**Analysis:** This graph serves as a progress indicator (KPI). It shows that, for these specific runs, only a small portion of the video has been processed, indicating an early testing or initial annotation phase.

#### D. Correlations and Density (Scatter & Contour Plots)

To understand detection distribution, scatter plots are used.

**Distribution comparison:**

![AI Scatter Plot](/Simple-Human-in-the-Loop-ML-Interface/assets/evaluated%20frame%20vs%20ia%20boxes.png)  
*Figure 5a: AI scatter plot (Evaluated Frames vs AI Boxes)*

![Human Scatter Plot](/Simple-Human-in-the-Loop-ML-Interface/assets/evaluated%20frame%20human%20boxes.png)  
*Figure 5b: Human scatter plot (Evaluated Frames vs Human Boxes)*

Finally, the **Contour Plot** provides a three-dimensional synthesis of interactions:

![Final Contour Plot](/Simple-Human-in-the-Loop-ML-Interface/assets/final%20cap.png)  
*Figure 6: Contour plot analyzing the density of the relationship between Evaluated Frames (X), AI Boxes (Y), and Human Boxes (Z/Color).*

**Analysis Conclusion:** The visualizations confirm an expected positive correlation: as the user evaluates more frames, the number of detected objects (AI and Human) increases. However, the consistent disparity between AI and human counts validates the necessity of this tool to *audit* the model before any retraining phase.

---

## 6. Roadmap & Future Integrations

### 6.1 Docker – Containerization
- Full containerization of the backend, database, and AI engine.
- Deployment via `docker-compose` to simplify setup and continuous integration.

### 6.2 MLflow – Model Tracking and Management
- Tracking parameters and metrics for each model and annotation session.
- Model versioning via the **Model Registry**.

### 6.3 DVC – Data Management and Versioning
- Versioning images and annotations without overloading the Git repository.
- Complete modification history to support future retraining.

---

---

## 8. Experiment Reproducibility and System Execution

This section presents visual evidence of system execution, data versioning, and user interaction through screenshots of DVC, the frontend interface, and Docker Compose runtime.

---

### 8.1 DVC – Data Versioning and Tracking

The project integrates **Data Version Control (DVC)** to manage large datasets such as images and annotation files without overloading the Git repository.  
DVC ensures full traceability of dataset evolution and enables reproducible experiments.

![DVC status and tracked files](/Simple-Human-in-the-Loop-ML-Interface/assets/finish%20push%20dvc.png)  
*Figure 7: DVC tracking status showing versioned datasets and annotation files.*



**Analysis:**  
DVC allows seamless synchronization between dataset versions and ML experiments logged in MLflow. This guarantees that every model run can be reproduced with the exact same data state.

---

### 8.2 Frontend – Human Annotation Interface

The frontend provides an interactive and intuitive interface allowing the human annotator to efficiently review and correct AI predictions.


![Frontend review mode](/Simple-Human-in-the-Loop-ML-Interface/assets/humane%20in%20the%20loop%20app.png)  
*Figure 8: Review mode displaying AI-generated predictions .*

**Analysis:**  
These interfaces demonstrate the central role of the human in the loop. The annotator actively supervises, corrects, and validates AI outputs, ensuring high-quality labeled data.

---

### 8.3 Docker Compose – System Deployment and Execution

The entire platform is orchestrated using **Docker Compose**, enabling seamless deployment of all services including the frontend, backend, MLflow tracking server, and supporting components.

![Docker Compose running services](/Simple-Human-in-the-Loop-ML-Interface/assets/docker_compose_up.png)  
*Figure 9: Successful execution of `docker-compose up` showing all services running.*


**Analysis:**  
Docker Compose simplifies system setup, ensures environment consistency, and allows rapid deployment across different machines, making the platform production-ready and reproducible.

---



## 9. Conclusion

This project provides a solid foundation for a modern **Human-in-the-Loop platform**, combining human interaction, artificial intelligence, Docker containerization, and MLOps best practices through MLflow and DVC.  
It enables full annotation traceability, AI model supervision, and continuous improvement of agent performance.
